
# üìò Hour 2 ‚Äì Data Cleaning & Preprocessing


## 1. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πâ‡∏ô

### A. Missing Data (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ)

#### üîπ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values

1. **‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß/‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (Drop)**

* ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠ missing ‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å (<5%)

```python
df = df.dropna()              # ‡∏•‡∏ö‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ NaN
df = df.dropna(axis=1)        # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ NaN
```

2. **‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ (Mean / Median / Mode Imputation)**

* **Mean Imputation:**
  $x_i = \frac{1}{n} \sum_{j=1}^n x_j$

* **Median Imputation:** ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏Å‡∏•‡∏≤‡∏á‡πÅ‡∏ó‡∏ô missing

* **Mode Imputation:** ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

```python
df["Age"].fillna(df["Age"].mean(), inplace=True)     # mean
df["Age"].fillna(df["Age"].median(), inplace=True)   # median
df["City"].fillna(df["City"].mode()[0], inplace=True) # mode
```

3. **Interpolation (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ñ‡πà‡∏≤)**

* ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏≠‡∏ö ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏≤ missing
* **‡∏™‡∏°‡∏Å‡∏≤‡∏£ Linear Interpolation:**
  $x_{t} = x_{t-1} + \frac{(x_{t+1} - x_{t-1})}{(t+1 - (t-1))} \times (t - (t-1))$

```python
df["Temperature"] = df["Temperature"].interpolate(method="linear")
```

4. **Regression Imputation**

* ‡πÉ‡∏ä‡πâ **‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô (Linear Regression)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ missing
* **‡∏™‡∏°‡∏Å‡∏≤‡∏£:**
  $\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n$

```python
from sklearn.linear_model import LinearRegression

X = df[["Age","Experience"]]    # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏¥‡∏™‡∏£‡∏∞
y = df["Salary"]                # ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
model = LinearRegression().fit(X.dropna(), y.dropna())
df.loc[df["Salary"].isnull(), "Salary"] = model.predict(X[df["Salary"].isnull()])
```


 5. **KNN Imputation**

* ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á **K ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î**
* **‡∏™‡∏°‡∏Å‡∏≤‡∏£:**
  $x_i = \frac{1}{k} \sum_{j \in N_k(i)} x_j$

```python
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=3)
df_filled = imputer.fit_transform(df)
```
 

### B. Feature Transformation (‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞)


#### üîπ Scaling

1. **Min-Max Scaling (Normalization)**

* **‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î**: ‡∏¢‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á $[0,1]$
* **‡∏™‡∏°‡∏Å‡∏≤‡∏£:**

  $$
  x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
  $$

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df["Salary_norm"] = scaler.fit_transform(df[["Salary"]])
```

2. **Standardization (Z-score)**

* **‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î**: ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (mean) = 0 ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ö‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏ö‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô (std) = 1
* **‡∏™‡∏°‡∏Å‡∏≤‡∏£:**

  $$
  z = \frac{x - \mu}{\sigma}
  $$

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df["Salary_std"] = scaler.fit_transform(df[["Salary"]])
```
**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**


| Name    | Salary | Salary\_MinMax | Salary\_Zscore |
| ------- | ------ | -------------- | -------------- |
| Alice   | 50,000 | 0.33           | -0.26          |
| Bob     | 60,000 | 1.00           | 1.30           |
| Charlie | 45,000 | 0.00           | -1.04          |

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢**

* **Salary (‡∏î‡∏¥‡∏ö):** ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏à‡∏£‡∏¥‡∏á ‚Üí scale ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å
* **Salary\_MinMax (0‚Äì1):** ‡∏¢‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0 ‡∏ñ‡∏∂‡∏á 1 ‚Üí ‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö neural network ‡∏´‡∏£‡∏∑‡∏≠ distance-based methods (KNN, clustering)
* **Salary\_Zscore:** ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ mean = 0, std = 1 ‚Üí ‡∏î‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö linear models, PCA


#### üîπ Encoding

1. **Label Encoding**

* **‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î**: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‚Üí ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö
* ‡πÄ‡∏ä‡πà‡∏ô `["Bangkok","Phuket","Chiang Mai"]` ‚Üí `[0,1,2]`

```python
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df["City_encoded"] = le.fit_transform(df["City"])
```

2. **One-Hot Encoding**

* **‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î**: ‡πÅ‡∏õ‡∏•‡∏á category ‚Üí binary columns
* ‡πÄ‡∏ä‡πà‡∏ô `City = ["Bangkok","Phuket"]` ‚Üí `Bangkok=1, Phuket=0`

```python
df = pd.get_dummies(df, columns=["City"])
```
**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á**

| Name    | City       | City\_Label | City\_Bangkok | City\_ChiangMai | City\_Phuket |
| ------- | ---------- | ----------- | ------------- | --------------- | ------------ |
| Alice   | Bangkok    | 0           | 1             | 0               | 0            |
| Bob     | Phuket     | 2           | 0             | 0               | 1            |
| Charlie | Chiang Mai | 1           | 0             | 1               | 0            |


**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢**

* **City\_Label** ‚Üí ‡πÉ‡∏ä‡πâ **Label Encoding** (Bangkok=0, Chiang Mai=1, Phuket=2)
* **City\_Bangkok / City\_ChiangMai / City\_Phuket** ‚Üí ‡πÉ‡∏ä‡πâ **One-Hot Encoding** (binary columns)

#### üîπ Date/Time Features

1. **Extract Components**

* ‡πÅ‡∏¢‡∏Å‡∏õ‡∏µ ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô ‡∏ß‡∏±‡∏ô ‡∏ß‡∏±‡∏ô‡πÉ‡∏ô‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á

```python
df["JoinDate"] = pd.to_datetime(df["JoinDate"])
df["Year"] = df["JoinDate"].dt.year
df["Month"] = df["JoinDate"].dt.month
df["Day"] = df["JoinDate"].dt.day
df["Weekday"] = df["JoinDate"].dt.weekday   # 0=Monday
df["Hour"] = df["JoinDate"].dt.hour
```

2. **‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡πÉ‡∏´‡∏°‡πà**

* **is\_weekend:**

  $$
  is\_weekend = 
  \begin{cases} 
  1 & \text{if weekday ‚àà \{5,6\}} \\ 
  0 & \text{otherwise} 
  \end{cases}
  $$

```python
df["IsWeekend"] = df["JoinDate"].dt.weekday >= 5
```

* **Season (‡∏§‡∏î‡∏π)** ‚Äì ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (‡πÑ‡∏ó‡∏¢: Summer=3‚Äì5, Rainy=6‚Äì10, Winter=11‚Äì2)

```python
def season(month):
    if month in [3,4,5]:
        return "Summer"
    elif month in [6,7,8,9,10]:
        return "Rainy"
    else:
        return "Winter"

df["Season"] = df["JoinDate"].dt.month.apply(season)
```
 
#### ‡∏™‡∏£‡∏∏‡∏õ

* **Scaling** ‚Üí ‡∏ó‡∏≥‡πÉ‡∏´‡πâ numerical features ‡∏°‡∏µ scale ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• converge ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô)
* **Encoding** ‚Üí ‡πÅ‡∏õ‡∏•‡∏á categorical features ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ
* **Date/Time Features** ‚Üí ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏û‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Date/Time Feature Extraction**

| Name    | JoinDate   | Year | Month | Day | Weekday | IsWeekend | Season |
| ------- | ---------- | ---- | ----- | --- | ------- | --------- | ------ |
| Alice   | 2020-01-15 | 2020 | 1     | 15  | 2 (Wed) | 0         | Winter |
| Bob     | 2020-07-20 | 2020 | 7     | 20  | 0 (Mon) | 0         | Rainy  |
| Charlie | 2020-12-05 | 2020 | 12    | 5   | 5 (Sat) | 1         | Winter |

**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢**

* **Year / Month / Day** ‚Üí ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å `JoinDate` ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
* **Weekday** ‚Üí ‡∏Ñ‡πà‡∏≤ 0‚Äì6 (0=Monday, ‚Ä¶, 6=Sunday)
* **IsWeekend** ‚Üí 1 ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡πÄ‡∏™‡∏≤‡∏£‡πå‚Äì‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå, 0 ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
* **Season** ‚Üí ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏≠‡∏á (‡πÑ‡∏ó‡∏¢: Summer=3‚Äì5, Rainy=6‚Äì10, Winter=11‚Äì2)
 
 

### C. Unstructured Data

### üîπ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Raw Text)

```
"Data Science is FUN!!! Data science helps in decision making."
```

---

### 1. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Text Cleaning)

* **Lowercasing** ‚Üí ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
* **Stopword Removal** ‚Üí ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô "is", "in", "the"
* **Punctuation Removal** ‚Üí ‡∏•‡∏ö‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå ‡πÄ‡∏ä‡πà‡∏ô `!`, `.`

```python
import re
import nltk
from nltk.corpus import stopwords

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

text = "Data Science is FUN!!! Data science helps in decision making."

# Lowercasing
text = text.lower()

# Remove punctuation
text = re.sub(r"[^a-zA-Z\s]", "", text)

# Remove stopwords
tokens = [word for word in text.split() if word not in stop_words]
print(tokens)
```

‚úÖ Output:

```
['data', 'science', 'fun', 'data', 'science', 'helps', 'decision', 'making']
```

---

### 2. Tokenization

* ‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥ (tokens)

```python
from nltk.tokenize import word_tokenize
nltk.download("punkt")

tokens = word_tokenize(text)
print(tokens)
```

‚úÖ Output:

```
['data', 'science', 'fun', 'data', 'science', 'helps', 'decision', 'making']
```

---

### 3. Bag-of-Words (BoW)

* ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (word frequency)

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "Data science is fun",
    "Science helps in decision making"
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)

print(vectorizer.get_feature_names_out())
print(X.toarray())
```

‚úÖ Output:

| Word | data | decision | fun | helps | in | is | making | science |
| ---- | ---- | -------- | --- | ----- | -- | -- | ------ | ------- |
| Doc1 | 1    | 0        | 1   | 0     | 0  | 1  | 0      | 1       |
| Doc2 | 0    | 1        | 0   | 1     | 1  | 0  | 1      | 1       |

---

### 4. TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)

* ‡πÉ‡∏ä‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏Ñ‡∏≥ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
* **‡∏™‡∏°‡∏Å‡∏≤‡∏£:**

$$
TFIDF(t,d) = TF(t,d) \times \log \frac{N}{DF(t)}
$$

* $TF(t,d)$ = ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ t ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ d
* $DF(t)$ = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥ t
* $N$ = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)

print(vectorizer.get_feature_names_out())
print(X.toarray())
```

‚úÖ Output (‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0‚Äì1 ‡πÅ‡∏ó‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥)

| Word | data | decision | fun  | helps | in   | is   | making | science |
| ---- | ---- | -------- | ---- | ----- | ---- | ---- | ------ | ------- |
| Doc1 | 0.58 | 0.00     | 0.58 | 0.00  | 0.00 | 0.58 | 0.00   | 0.45    |
| Doc2 | 0.00 | 0.50     | 0.00 | 0.50  | 0.50 | 0.00 | 0.50   | 0.38    |

---

## ‚úÖ ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ô‡∏µ‡πâ)

1. ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á **Structured vs Unstructured Data**
2. ‡∏Å‡∏≤‡∏£ Clean ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (lowercasing, stopwords, punctuation removal)
3. ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô features ‡∏î‡πâ‡∏ß‡∏¢ **Tokenization ‚Üí Bag-of-Words ‚Üí TF-IDF**

 

## 2. Hands-on Exercises 

### Structure Data

```python
# ========================================
# STEP 0: Import Libraries
# ========================================
import pandas as pd
import numpy as np

# ========================================
# STEP 1: Load Dataset
# ========================================
file_path = "messy_data.csv"   # ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á/Colab
df = pd.read_csv(file_path)

print("üîπ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å:")
print(df.head())
print("\nüîπ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô:")
print(df.info())
```

---

```python
# ========================================
# STEP 2: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Missing Values
# ========================================
print("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Missing values ‡∏ï‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå:")
print(df.isnull().sum())

# ‡πÅ‡∏™‡∏î‡∏á % missing
print("\n‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå Missing:")
print((df.isnull().mean() * 100).round(2))
```

---

```python
# ========================================
# STEP 3: Handling Missing Data
# ========================================

# Age ‚Üí ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
df["Age"].fillna(df["Age"].mean(), inplace=True)

# City ‚Üí ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ mode (‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
df["City"].fillna(df["City"].mode()[0], inplace=True)

# Salary ‚Üí ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ median
df["Salary"].fillna(df["Salary"].median(), inplace=True)

# Comments ‚Üí ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ "Unknown"
df["Comments"].fillna("Unknown", inplace=True)
```

---

```python
# ========================================
# STEP 4: Clean Text (Comments)
# ========================================
import re

def clean_text(text):
    text = str(text).lower()                       # lowercase
    text = re.sub(r"[^a-z\s]", "", text)           # ‡∏•‡∏ö punctuation/‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    return text.strip()

df["Comments_clean"] = df["Comments"].apply(clean_text)

print(df[["Comments", "Comments_clean"]].head(10))
```

---

```python
# ========================================
# STEP 5: Remove Duplicates
# ========================================
before = df.shape[0]
df.drop_duplicates(inplace=True)
after = df.shape[0]

print(f"‡∏•‡∏ö duplicates ‡πÅ‡∏•‡πâ‡∏ß: {before - after} records ‡∏ñ‡∏π‡∏Å‡∏•‡∏ö")
```

---

```python
# ========================================
# STEP 6: Feature Transformation
# ========================================

from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder

# ---- Scaling ----
scaler = MinMaxScaler()
df["Salary_MinMax"] = scaler.fit_transform(df[["Salary"]])

scaler = StandardScaler()
df["Salary_Zscore"] = scaler.fit_transform(df[["Salary"]])

# ---- Encoding ----
le = LabelEncoder()
df["Department_Label"] = le.fit_transform(df["Department"])

df = pd.get_dummies(df, columns=["City"], prefix="City")

# ---- Date/Time ----
df["JoinDate"] = pd.to_datetime(df["JoinDate"])
df["Year"] = df["JoinDate"].dt.year
df["Month"] = df["JoinDate"].dt.month
df["Day"] = df["JoinDate"].dt.day
df["Weekday"] = df["JoinDate"].dt.weekday
df["IsWeekend"] = (df["JoinDate"].dt.weekday >= 5).astype(int)
```

---

```python
# ========================================
# STEP 7: Feature Engineering - Season
# ========================================
def season(month):
    if month in [3,4,5]:
        return "Summer"
    elif month in [6,7,8,9,10]:
        return "Rainy"
    else:
        return "Winter"

df["Season"] = df["Month"].apply(season)
```

---

```python
# ========================================
# STEP 8: Export Ready-to-Use Dataset
# ========================================
output_file = "cleaned_ready_data.csv"
df.to_csv(output_file, index=False)

print(f"‚úÖ Preprocessed dataset saved as {output_file}")
```

---

### ‚úÖ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

1. ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ **Missing Values** (drop, fillna ‡∏î‡πâ‡∏ß‡∏¢ mean/median/mode)
2. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (text cleaning)
3. ‡∏Å‡∏≤‡∏£‡∏•‡∏ö **Duplicates**
4. **Scaling & Encoding** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡πÉ‡∏´‡πâ numerical/categorical features ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ
5. **Date/Time Feature Engineering** (Year, Month, Day, Weekday, IsWeekend, Season)
6. ‡∏Å‡∏≤‡∏£ export dataset ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (**Ready-to-Use File**)


## Unstructure data (Text)

```python
# ========================================
# STEP 0: Import Libraries
# ========================================
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix
import re
```

---

```python
# ========================================
# STEP 1: Load Dataset
# ========================================
file_path = "sms_spam_ham.csv"   # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô path ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå
df = pd.read_csv(file_path)

print("üîπ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å:")
print(df.head())
print("\n‡∏à‡∏≥‡∏ô‡∏ß‡∏ô spam:", (df["Label"]=="spam").sum())
print("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô ham:", (df["Label"]=="ham").sum())
```

---

```python
# ========================================
# STEP 2: Text Preprocessing
# ========================================
def clean_text(text):
    text = text.lower()                         # lowercase
    text = re.sub(r"[^a-z\s]", "", text)        # ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå
    return text.strip()

df["Clean_Message"] = df["Message"].apply(clean_text)

print(df[["Message", "Clean_Message"]].head(10))
```

---

```python
# ========================================
# STEP 3: Split Train/Test
# ========================================
X = df["Clean_Message"]
y = df["Label"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("‡∏Ç‡∏ô‡∏≤‡∏î Training set:", len(X_train))
print("‡∏Ç‡∏ô‡∏≤‡∏î Test set:", len(X_test))
```

---

```python
# ========================================
# STEP 4: TF-IDF Vectorization
# ========================================
vectorizer = TfidfVectorizer(stop_words="english")

X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

print("‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö TF-IDF:", X_train_tfidf.shape)
```

---

```python
# ========================================
# STEP 5: Train Naive Bayes Classifier
# ========================================
model = MultinomialNB()
model.fit(X_train_tfidf, y_train)

y_pred = model.predict(X_test_tfidf)
```

---

```python
# ========================================
# STEP 6: Evaluation
# ========================================
print("üîπ Classification Report:")
print(classification_report(y_test, y_pred))

print("üîπ Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
```

---

### ‚úÖ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

1. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ **Text Cleaning** (lowercasing, remove punctuation)
2. ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô **TF-IDF features**
3. ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á dataset ‡πÄ‡∏õ‡πá‡∏ô **Train/Test**
4. ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ train **Naive Bayes Classifier** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô text classification
5. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ **Confusion Matrix ‡πÅ‡∏•‡∏∞ Classification Report**

