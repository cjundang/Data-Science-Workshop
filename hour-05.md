# Hour 5 : Usecase
## Case 1: ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡πÑ‡∏≠‡∏®‡∏Å‡∏£‡∏µ‡∏°‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô


1. ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (Objective)

‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ **‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô (sales\_thb)** ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®, ‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î ‡πÅ‡∏•‡∏∞‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤

* **Target (y):** sales\_thb (‡∏ö‡∏≤‡∏ó)
* **Features (X):** temperature\_C, humidity\_pct, month, is\_weekend, is\_holiday, promo\_budget\_thb, foot\_traffic, prior\_day\_sales, date\_index


2. ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (EDA)

    1. **Distribution ‡∏Ç‡∏≠‡∏á‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢:**

    * ‡πÉ‡∏ä‡πâ Histogram/KDE ‚Üí ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ skew ‡∏´‡∏£‡∏∑‡∏≠ outlier ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    * ‡∏ñ‡πâ‡∏≤ skew ‡∏°‡∏≤‡∏Å ‡∏≠‡∏≤‡∏à‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ log-transform

    2. **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á feature ‡∏Å‡∏±‡∏ö‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢:**

    * **temperature\_C vs sales\_thb:** ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡πâ‡∏á (‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡πÄ‡∏¢‡πá‡∏ô‡∏Ç‡∏≤‡∏¢‡∏ô‡πâ‡∏≠‡∏¢, ‡∏£‡πâ‡∏≠‡∏ô‡∏à‡∏±‡∏î‡∏Ç‡∏≤‡∏¢‡∏°‡∏≤‡∏Å ‚Üí non-linear)
    * **foot\_traffic vs sales\_thb:** ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô (‡∏Ñ‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πâ‡∏≤‡∏ô‡∏°‡∏≤‡∏Å ‚Üí ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏™‡∏π‡∏á)
    * **promo\_budget\_thb vs sales\_thb:** ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå, ‡∏≠‡∏≤‡∏à‡∏°‡∏µ diminishing return

    3. **Seasonality/Trend:**

    * month, is\_holiday, is\_weekend ‚Üí ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    * prior\_day\_sales ‚Üí ‡πÉ‡∏ä‡πâ‡∏à‡∏±‡∏ö autocorrelation


3. ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (Modeling Strategy)

    1. **Baseline:**

    * ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î = ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    * ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö

    2. **Linear Regression:**

    * ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô $\hat{y} = \beta\_0 + \beta\_1 x\_1 + \cdots + \beta\_k x\_k$
    * ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢

    3. **Polynomial Features (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ temperature\_C):**

    * ‡πÄ‡∏û‡∏¥‡πà‡∏° $temperature^2$ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à non-linearity

    4. **Tree-based Models (Decision Tree, Random Forest):**

    * ‡πÉ‡∏ä‡πâ‡∏ñ‡πâ‡∏≤ EDA ‡∏û‡∏ö non-linear ‡πÅ‡∏•‡∏∞ interaction ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô


4. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• (Evaluation)

‡πÉ‡∏ä‡πâ **Train/Test Split (‡πÄ‡∏ä‡πà‡∏ô 80/20)** ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢

* **MAE (Mean Absolute Error):** ‡∏Ñ‡πà‡∏≤ error ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏ö‡∏≤‡∏ó ‚Üí ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
* **RMSE (Root Mean Squared Error):** ‡∏•‡∏á‡πÇ‡∏ó‡∏© error ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà ‚Üí ‡πÉ‡∏ä‡πâ‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏°‡∏µ outlier ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∏‡∏î‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏û‡∏•‡∏≤‡∏î‡∏´‡∏ô‡∏±‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
* **$R^2$ (Coefficient of Determination):** ‡∏ß‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏Å‡∏µ‡πà %

‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:

* **Residual Plot:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ error ‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡∏ö random ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ pattern (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‚Üí ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°)


5. ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏ú‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (Interpretation)

* ‡∏ñ‡πâ‡∏≤ **MAE = 500 ‡∏ö‡∏≤‡∏ó** ‚Üí ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ \~500 ‡∏ö‡∏≤‡∏ó‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô
* ‡∏ñ‡πâ‡∏≤ **RMSE = 700 ‡∏ö‡∏≤‡∏ó** ‡πÅ‡∏•‡∏∞‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ MAE ‡∏°‡∏≤‡∏Å ‚Üí ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ö‡∏≤‡∏á‡∏ß‡∏±‡∏ô error ‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å (‡πÄ‡∏ä‡πà‡∏ô ‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏¢‡∏≤‡∏ß/‡∏û‡∏¥‡πÄ‡∏®‡∏©)
* ‡∏ñ‡πâ‡∏≤ **$R^2 = 0.85\$** ‚Üí ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡πÑ‡∏î‡πâ 85% ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏î‡∏µ‡∏°‡∏≤‡∏Å
* **‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° feature:**

  * $\beta\_{temperature} > 0$ ‚Üí ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
  * $\beta\_{promo\_budget} > 0$ ‚Üí ‡∏á‡∏ö‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô‡∏°‡∏≤‡∏Å ‚Üí ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
  * $\beta\_{is\_weekend}\$ ‡∏ö‡∏ß‡∏Å ‚Üí ‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏Ç‡∏≤‡∏¢‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏ß‡∏±‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤

**‡∏™‡∏£‡∏∏‡∏õ Workflow**

1. **EDA:** ‡∏ï‡∏£‡∏ß‡∏à distribution, correlation, seasonality
2. **Baseline:** ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢ ‚Üí ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
3. **Model:** Linear Regression ‚Üí Polynomial (temp¬≤) ‚Üí Random Forest (‡∏ñ‡πâ‡∏≤ non-linear ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)
4. **Evaluation:** MAE, RMSE, R¬≤ + Residual Plot
5. **Interpretation:** ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à (‡πÄ‡∏ä‡πà‡∏ô ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏£‡πâ‡∏≠‡∏ô + ‡∏á‡∏ö‡πÇ‡∏õ‡∏£‡∏™‡∏π‡∏á = ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏û‡∏∏‡πà‡∏á)

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# -----------------------------
# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥ (200 records)
# -----------------------------
np.random.seed(42)
n = 200
date_index = pd.date_range("2023-01-01", periods=n, freq="D")

temperature = np.random.normal(30, 5, n)          # ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ (¬∞C)
humidity = np.random.uniform(40, 90, n)           # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏∑‡πâ‡∏ô (%)
month = date_index.month
is_weekend = (date_index.weekday >= 5).astype(int)
is_holiday = np.random.binomial(1, 0.1, n)        # 10% ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏û‡∏¥‡πÄ‡∏®‡∏©
promo_budget = np.random.choice([0, 500, 1000, 2000], n, p=[0.4, 0.3, 0.2, 0.1])
foot_traffic = np.random.poisson(lam=200, size=n) # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πâ‡∏≤‡∏ô
prior_day_sales = np.random.normal(5000, 1000, n)

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡∏à‡∏£‡∏¥‡∏á (target)
sales = (
    2000
    + temperature * 120
    - humidity * 15
    + promo_budget * 1.5
    + foot_traffic * 10
    + prior_day_sales * 0.4
    + is_weekend * 500
    + is_holiday * 1000
    + np.random.normal(0, 800, n)  # noise
)

df = pd.DataFrame({
    "date_index": date_index,
    "temperature_C": temperature,
    "humidity_pct": humidity,
    "month": month,
    "is_weekend": is_weekend,
    "is_holiday": is_holiday,
    "promo_budget_thb": promo_budget,
    "foot_traffic": foot_traffic,
    "prior_day_sales": prior_day_sales,
    "sales_thb": sales
})

print(df.head())

# -----------------------------
# 2. EDA ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
# -----------------------------
plt.figure(figsize=(10,4))
df["sales_thb"].hist(bins=20)
plt.title("Distribution of Daily Ice Cream Sales")
plt.xlabel("Sales (THB)")
plt.ylabel("Frequency")
plt.show()

plt.scatter(df["temperature_C"], df["sales_thb"], alpha=0.6)
plt.title("Temperature vs Sales")
plt.xlabel("Temperature (¬∞C)")
plt.ylabel("Sales (THB)")
plt.show()

plt.scatter(df["foot_traffic"], df["sales_thb"], alpha=0.6, color="orange")
plt.title("Foot Traffic vs Sales")
plt.xlabel("Foot Traffic")
plt.ylabel("Sales (THB)")
plt.show()

# -----------------------------
# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• (Baseline vs Linear Regression)
# -----------------------------
X = df[["temperature_C","humidity_pct","month","is_weekend","is_holiday",
        "promo_budget_thb","foot_traffic","prior_day_sales"]]
y = df["sales_thb"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# baseline = ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
baseline_pred = np.repeat(y_train.mean(), len(y_test))

# Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

# -----------------------------
# 4. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
# -----------------------------
def evaluate(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    print(f"{model_name} -> MAE: {mae:.2f}, RMSE: {rmse:.2f}, R¬≤: {r2:.3f}")

evaluate(y_test, baseline_pred, "Baseline")
evaluate(y_test, y_pred, "Linear Regression")

# -----------------------------
# 5. Residual Plot
# -----------------------------
residuals = y_test - y_pred
plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.title("Residual Plot")
plt.xlabel("Predicted Sales")
plt.ylabel("Residuals")
plt.show()
```


## Case 2: Signup Conversion Prediction

### 1. ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (Objective)

* ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô session ‡∏à‡∏∞ **‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (signed\_up=1)** ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (signed\_up=0)
* ‡πÉ‡∏ä‡πâ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏î‡πâ‡∏≤‡∏ô‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô, ‡πÅ‡∏´‡∏•‡πà‡∏á referral, device ‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ



### 2. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Dataset)

* **Features (X):**

  * *session\_duration\_s:* ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô session
  * *pages\_viewed:* ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏¥‡∏î
  * *device:* mobile/desktop/tablet
  * *referral:* ads/seo/email/direct
  * *country:* TH/VN/ID/SG
  * *is\_weekend:* ‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
  * *clicks\_cta:* ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° call-to-action
  * *returning\_user:* ‡πÄ‡∏Ñ‡∏¢‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

* **Label (y):** `signed_up ‚àà {0,1}`

* **Sample size:** 2,000 records

### 3. ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (EDA)

1. **Rate ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (conversion rate):**

   * ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏° `device` ‚Üí mobile ‡∏≠‡∏≤‡∏à‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ desktop
   * ‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏° `referral` ‚Üí traffic ‡∏à‡∏≤‡∏Å email/ads ‡∏°‡∏±‡∏Å conversion ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ direct
2. **Distribution:**

   * *session\_duration\_s:* ‡∏≠‡∏≤‡∏à skew (‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏±‡πâ‡∏ô, ‡∏ö‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å)
   * *pages\_viewed:* ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏¢‡∏≠‡∏∞‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö signup ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
3. **Correlation:**

   * `clicks_cta` ‡πÅ‡∏•‡∏∞ `session_duration_s` ‡∏°‡∏±‡∏Å‡∏°‡∏µ positive correlation ‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£ signup


### 4. Baseline Model

* **Predict majority class:** ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤ 70% ‡∏Ç‡∏≠‡∏á session ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏±‡∏Ñ‡∏£ ‚Üí baseline accuracy = 70%
* ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏ß‡πà‡∏≤‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö ‚Äú‡πÄ‡∏î‡∏≤‡∏™‡∏∏‡πà‡∏°‚Äù ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà



### 5. ‡πÇ‡∏°‡πÄ‡∏î‡∏• (Modeling Strategy)

1. **Logistic Regression (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô):**

   * One-hot encode categorical features (device, referral, country)
   * ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° coefficient ‡πÄ‡∏õ‡πá‡∏ô log-odds ‚Üí ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤ feature ‡πÑ‡∏´‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏≠‡∏Å‡∏≤‡∏™ signup

2. **Tree-based (Decision Tree / Random Forest):**

   * ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö non-linearity ‡πÅ‡∏•‡∏∞ interaction ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
   * ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ feature importance

### 6. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (Evaluation)

* **Confusion Matrix:** ‡πÅ‡∏™‡∏î‡∏á TP, TN, FP, FN
* **Accuracy:** ‡∏ß‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏° (‡∏î‡∏µ‡πÄ‡∏°‡∏∑‡πà‡∏≠ class balance)
* **Precision:** ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô signup ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤ signup ‡πÅ‡∏•‡πâ‡∏ß‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á
* **Recall:** ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô signup ‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ
* **F1-score:** ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Precision ‡πÅ‡∏•‡∏∞ Recall
* **ROC-AUC:** ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å positive/negative ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö threshold

‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:

* **Threshold analysis:** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å threshold ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö business goal ‡πÄ‡∏ä‡πà‡∏ô

  * ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å ‚Äú‡∏•‡∏î false positive‚Äù (‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏ï‡∏≤‡∏°‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏à‡∏£‡∏¥‡∏á) ‚Üí ‡πÄ‡∏ô‡πâ‡∏ô **Precision**
  * ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å ‚Äú‡∏à‡∏±‡∏ö‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‚Äù ‚Üí ‡πÄ‡∏ô‡πâ‡∏ô **Recall**



### 7. ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏ú‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ (Interpretation)

* ‡∏ñ‡πâ‡∏≤ **Accuracy = 85%** ‡πÅ‡∏ï‡πà dataset imbalance (90% ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏±‡∏Ñ‡∏£, 10% ‡∏™‡∏°‡∏±‡∏Ñ‡∏£) ‚Üí ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏ô‡∏±‡∏Å ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π Precision/Recall ‡∏î‡πâ‡∏ß‡∏¢
* ‡∏ñ‡πâ‡∏≤ **Precision = 0.80, Recall = 0.60** ‚Üí 80% ‡∏Ç‡∏≠‡∏á‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏à‡∏£‡∏¥‡∏á‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏ï‡πà‡∏à‡∏±‡∏ö‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏°‡∏≤‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà 60%
* ‡∏ñ‡πâ‡∏≤ **F1 = 0.69** ‚Üí ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Å‡∏•‡∏≤‡∏á ‡πÜ ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Precision ‡πÅ‡∏•‡∏∞ Recall
* ‡∏ñ‡πâ‡∏≤ **AUC = 0.90** ‚Üí ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏¢‡∏Å ‚Äúsignup vs not signup‚Äù ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏°‡∏≤‡∏Å


### üîπ ‡∏™‡∏£‡∏∏‡∏õ Workflow

1. **EDA:** Conversion rate ‡∏ï‡∏≤‡∏° device/referral/country, distribution ‡∏Ç‡∏≠‡∏á session duration, correlation ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì
2. **Baseline:** ‡∏ó‡∏≤‡∏¢ majority class
3. **Model:** Logistic Regression (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô) ‚Üí Tree/Random Forest ‡∏û‡∏£‡πâ‡∏≠‡∏° one-hot encoding
4. **Evaluation:** Confusion Matrix, Accuracy, Precision, Recall, F1, ROC-AUC
5. **Threshold tuning:** ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Precision-Recall trade-off
6. **Interpretation:**

   * Logistic: coefficient/log-odds ‡πÅ‡∏õ‡∏•‡∏ï‡∏£‡∏á‡πÑ‡∏õ‡∏ï‡∏£‡∏á‡∏°‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô `clicks_cta` +1 ‚Üí ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°)
   * Tree/Forest: ‡πÉ‡∏ä‡πâ feature importance ‡∏î‡∏π‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ï‡πà‡∏≠ signup

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, roc_curve
)

# -----------------------------
# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥
# -----------------------------
np.random.seed(42)
n = 2000

session_duration = np.random.exponential(scale=120, size=n)   # ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
pages_viewed = np.random.poisson(lam=4, size=n)
device = np.random.choice(["mobile", "desktop", "tablet"], size=n, p=[0.6, 0.3, 0.1])
referral = np.random.choice(["ads", "seo", "email", "direct"], size=n, p=[0.3, 0.3, 0.2, 0.2])
country = np.random.choice(["TH", "VN", "ID", "SG"], size=n, p=[0.4, 0.2, 0.3, 0.1])
is_weekend = np.random.binomial(1, 0.3, size=n)
clicks_cta = np.random.poisson(lam=1, size=n)
returning_user = np.random.binomial(1, 0.4, size=n)

# target: signed_up
# ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏à‡∏≤‡∏Å‡∏ö‡∏≤‡∏á feature
logits = (
    0.001*session_duration
    + 0.2*pages_viewed
    + 0.5*clicks_cta
    + 0.8*returning_user
    + np.where(device=="desktop", 0.5, 0)
    + np.where(referral=="email", 0.7, 0)
    + np.where(is_weekend==1, -0.2, 0)
)
prob = 1 / (1 + np.exp(-logits))
signed_up = np.random.binomial(1, prob)

df = pd.DataFrame({
    "session_duration_s": session_duration,
    "pages_viewed": pages_viewed,
    "device": device,
    "referral": referral,
    "country": country,
    "is_weekend": is_weekend,
    "clicks_cta": clicks_cta,
    "returning_user": returning_user,
    "signed_up": signed_up
})

print(df.head())

# -----------------------------
# 2. Train/Test Split
# -----------------------------
X = df.drop("signed_up", axis=1)
y = df["signed_up"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -----------------------------
# 3. Preprocessing + Models
# -----------------------------
categorical = ["device", "referral", "country"]
numeric = ["session_duration_s", "pages_viewed", "clicks_cta", "is_weekend", "returning_user"]

preprocessor = ColumnTransformer([
    ("cat", OneHotEncoder(drop="first"), categorical),
    ("num", "passthrough", numeric)
])

# Logistic Regression pipeline
log_reg = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", LogisticRegression(max_iter=1000))
])

# Random Forest pipeline
rf = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", RandomForestClassifier(n_estimators=100, random_state=42))
])

# -----------------------------
# 4. Baseline
# -----------------------------
baseline_pred = np.repeat(y_train.mode()[0], len(y_test))

# -----------------------------
# 5. Training
# -----------------------------
log_reg.fit(X_train, y_train)
rf.fit(X_train, y_train)

y_pred_log = log_reg.predict(X_test)
y_prob_log = log_reg.predict_proba(X_test)[:,1]

y_pred_rf = rf.predict(X_test)
y_prob_rf = rf.predict_proba(X_test)[:,1]

# -----------------------------
# 6. Evaluation Function
# -----------------------------
def evaluate(y_true, y_pred, y_prob, model_name):
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    auc = roc_auc_score(y_true, y_prob)
    print(f"{model_name}:")
    print(f" Accuracy={acc:.3f}, Precision={prec:.3f}, Recall={rec:.3f}, F1={f1:.3f}, ROC-AUC={auc:.3f}")
    print("-"*50)

evaluate(y_test, baseline_pred, np.zeros(len(y_test)), "Baseline")
evaluate(y_test, y_pred_log, y_prob_log, "Logistic Regression")
evaluate(y_test, y_pred_rf, y_prob_rf, "Random Forest")

# -----------------------------
# 7. Confusion Matrix
# -----------------------------
cm = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Not Signup","Signup"], yticklabels=["Not Signup","Signup"])
plt.title("Confusion Matrix (Random Forest)")
plt.ylabel("True Label")
plt.xlabel("Predicted Label")
plt.show()

# -----------------------------
# 8. ROC Curve
# -----------------------------
fpr, tpr, _ = roc_curve(y_test, y_prob_rf)
plt.plot(fpr, tpr, label="Random Forest")
fpr2, tpr2, _ = roc_curve(y_test, y_prob_log)
plt.plot(fpr2, tpr2, label="Logistic Regression")
plt.plot([0,1],[0,1],'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()
```